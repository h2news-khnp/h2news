import json
from datetime import datetime
from pathlib import Path
import requests
from bs4 import BeautifulSoup

# -----------------------------
# 1. ê¸°ë³¸ ì„¤ì •
# -----------------------------

BASE_URL = "https://www.gasnews.com"

# ê°€ìŠ¤ì‹ ë¬¸ì˜ 'ìˆ˜ì†ŒÂ·ì—°ë£Œì „ì§€' ì¹´í…Œê³ ë¦¬
GASNEWS_LIST_URL = (
    "https://www.gasnews.com/news/articleList.html?"
    "page={page}&sc_section_code=S1N9&view_type="
)

# ì „ê¸°ì‹ ë¬¸
ELECTIMES_BASE_URL = "https://www.electimes.com"
ELECTIMES_LIST_URL = (
    "https://www.electimes.com/news/articleList.html?page={page}&view_type=sm"
)

# -----------------------------
# 2. ìˆ˜ì†Œ ê´€ë ¨ í‚¤ì›Œë“œ (ì „ ë§¤ì²´ ê³µí†µ)
# -----------------------------
HYDROGEN_KEYWORDS = [
    # ê¸°ë³¸
    "ìˆ˜ì†Œ", "ì—°ë£Œì „ì§€", "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ",
    "PAFC", "SOFC", "MCFC",

    # ìˆ˜ì „í•´/ì „í•´ì¡°
    "ìˆ˜ì „í•´", "ì „í•´ì¡°", "PEMEC", "AEM", "ì•Œì¹´ë¼ì¸",

    # ì•”ëª¨ë‹ˆì•„ ê¸°ë°˜
    "ì•”ëª¨ë‹ˆì•„", "ì•”ëª¨ë‹ˆì•„í¬ë˜í‚¹",

    # ì¸í”„ë¼ & ì •ì±…
    "ìˆ˜ì†Œìƒì‚°", "ìˆ˜ì†Œì €ì¥", "ì•¡í™”ìˆ˜ì†Œ",
    "ì¶©ì „ì†Œ", "ìˆ˜ì†Œë²„ìŠ¤", "ìˆ˜ì†Œì°¨", "ì¸í”„ë¼",

    # ê¸°ê´€/ê¸°ì—… í‚¤ì›Œë“œ
    "í•œìˆ˜ì›", "ë‘ì‚°í“¨ì–¼ì…€", "í•œí™”ì„íŒ©íŠ¸", "í˜„ëŒ€ì°¨",

    # ê¸°íƒ€
    "HPS", "HPC", "REC", "RPS",
]

def contains_hydrogen_keyword(text: str) -> bool:
    """ìˆ˜ì†Œ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¥¼ ê²€ì‚¬."""
    text = text.lower()
    return any(kw.lower() in text for kw in HYDROGEN_KEYWORDS)


# -----------------------------
# 3. ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
# -----------------------------

def normalize_gasnews_date(raw: str) -> str:
    """
    ex) '12.09 09:50' â†’ '2025-12-09'
    """
    raw = (raw or "").strip()
    year = datetime.now().year

    try:
        return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d %H:%M").strftime("%Y-%m-%d")
    except:
        try:
            return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d").strftime("%Y-%m-%d")
        except:
            return datetime.now().strftime("%Y-%m-%d")


def normalize_electimes_date(raw: str) -> str:
    raw = raw.strip()
    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d"):
        try:
            return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
        except:
            continue
    return datetime.now().strftime("%Y-%m-%d")


# -----------------------------
# 4. íƒœê·¸ ìƒì„±
# -----------------------------

def make_tags(title: str) -> list[str]:
    tags = [kw for kw in HYDROGEN_KEYWORDS if kw.lower() in title.lower()]
    return list(dict.fromkeys(tags))  # ì¤‘ë³µ ì œê±°


# -----------------------------
# 5. ê°€ìŠ¤ì‹ ë¬¸ í¬ë¡¤ëŸ¬
# -----------------------------

def crawl_gasnews(max_pages: int = 2) -> list[dict]:
    results = []

    for page in range(1, max_pages + 1):
        url = GASNEWS_LIST_URL.format(page=page)
        print(f"[ê°€ìŠ¤ì‹ ë¬¸] {page} í˜ì´ì§€ â†’ {url}")

        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("section#section-list ul.type1 > li"):
            title_a = li.select_one("h4.titles a")
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            article_url = BASE_URL + title_a.get("href", "")

            date_el = li.select_one("em.info.dated")
            date_str = normalize_gasnews_date(date_el.get_text(strip=True))

            # í•„í„°ë§
            if not contains_hydrogen_keyword(title):
                continue

            # â˜… ìƒì„¸ ë³¸ë¬¸ ì¶”ì¶œ
            body = extract_article_body(article_url)

            results.append({
                "date": date_str,
                "source": "ê°€ìŠ¤ì‹ ë¬¸",
                "title": title,
                "url": article_url,
                "body": body,
                "tags": make_tags(title)
            })

    return results



# -----------------------------
# 6. ì „ê¸°ì‹ ë¬¸ í¬ë¡¤ëŸ¬
# -----------------------------

def crawl_electimes(max_pages: int = 2) -> list[dict]:
    results = []

    for page in range(1, max_pages + 1):
        url = ELECTIMES_LIST_URL.format(page=page)
        print(f"[ì „ê¸°ì‹ ë¬¸] {page} í˜ì´ì§€ â†’ {url}")

        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("#section-list ul.type > li.item"):
            title_a = li.select_one("h4.titles a.replace-titles")
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            article_url = ELECTIMES_BASE_URL + title_a.get("href", "")

            date_el = li.select_one("em.replace-date")
            date_str = normalize_electimes_date(date_el.get_text(strip=True))

            summary_el = li.select_one("p.lead a.replace-read")
            summary = summary_el.get_text(strip=True) if summary_el else ""

            # í•„í„°ë§
            combined = f"{title} {summary}".lower()
            if not contains_hydrogen_keyword(combined):
                continue

            # â˜… ìƒì„¸ ë³¸ë¬¸ ì¶”ì¶œ ì¶”ê°€
            body = extract_article_body(article_url)

            results.append({
                "date": date_str,
                "source": "ì „ê¸°ì‹ ë¬¸",
                "title": title,
                "url": article_url,
                "summary": summary,
                "body": body,
                "tags": make_tags(title)
            })

    return results

# -----------------------------
# 7. ì¹´ë“œë‰´ìŠ¤
# -----------------------------

from cardnews_image import make_cardnews_image



# -----------------------------
# 8. ë©”ì¸ (JSON ì €ì¥)
# -----------------------------

def main():
    today = datetime.now().strftime("%Y-%m-%d")

    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    all_articles = []

    # ë‘ ì‹ ë¬¸ ë™ì‹œ í¬ë¡¤ë§
    all_articles.extend(crawl_gasnews(max_pages=3))
    all_articles.extend(crawl_electimes(max_pages=3))

    # ì˜¤ëŠ˜ ê¸°ì‚¬ë§Œ í•„í„°ë§
    today_articles = [a for a in all_articles if a["date"] == today]

    out_path = data_dir / f"{today}.json"
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(today_articles, f, ensure_ascii=False, indent=2)

    print(f"\nğŸŸ¢ ì €ì¥ ì™„ë£Œ: {len(today_articles)}ê±´ â†’ {out_path}\n")


# -----------------------------
# 8. ì‹¤í–‰
# -----------------------------
if __name__ == "__main__":
    main()
