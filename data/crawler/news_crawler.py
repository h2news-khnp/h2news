import json
import re
from datetime import datetime
from pathlib import Path
import requests
from bs4 import BeautifulSoup

# -------------------------------------------------------
# 1. ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------------

BASE_URL = "https://www.gasnews.com"

GASNEWS_LIST_URL = (
    "https://www.gasnews.com/news/articleList.html?"
    "page={page}&sc_section_code=S1N9&view_type="
)

ELECTIMES_BASE_URL = "https://www.electimes.com"
ELECTIMES_LIST_URL = "https://www.electimes.com/news/articleList.html?page={page}&view_type=sm"

# -------------------------------------------------------
# 2. ìˆ˜ì†Œ í‚¤ì›Œë“œ (ëª¨ë“  ì‹ ë¬¸ ê³µí†µ)
# -------------------------------------------------------

HYDROGEN_KEYWORDS = [
    "ìˆ˜ì†Œ", "ì—°ë£Œì „ì§€", "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ì›ìë ¥",
    "PAFC", "SOFC", "MCFC", "PEM", "ì¬ìƒ", "ë°°ì¶œê¶Œ", "íˆíŠ¸íŒí”„", "ë„ì‹œê°€ìŠ¤", "êµ¬ì—­ì „ê¸°", "PPA",
    "ìˆ˜ì „í•´", "ì „í•´ì¡°", "PEMEC", "AEM", "ì•Œì¹´ë¼ì¸", "ë¶„ì‚°", "NDC", "í•‘í¬ìˆ˜ì†Œ",
    "ì•”ëª¨ë‹ˆì•„", "ì•”ëª¨ë‹ˆì•„í¬ë˜í‚¹", "CCU", "CCUS", "ê¸°í›„ë¶€", "ESS", "ë°°í„°ë¦¬",
    "ìˆ˜ì†Œìƒì‚°", "ìˆ˜ì†Œì €ì¥", "ì•¡í™”ìˆ˜ì†Œ",
    "ì¶©ì „ì†Œ", "ìˆ˜ì†Œë²„ìŠ¤", "ìˆ˜ì†Œì°¨", "ì¸í”„ë¼",
    "í•œìˆ˜ì›", "ë‘ì‚°í“¨ì–¼ì…€", "í•œí™”ì„íŒ©íŠ¸", "í˜„ëŒ€ì°¨",
    "HPS", "HPC", "REC", "RPS"
]

def contains_hydrogen_keyword(text: str) -> bool:
    text = text.lower()
    return any(kw.lower() in text for kw in HYDROGEN_KEYWORDS)


# -------------------------------------------------------
# 3. ë‚ ì§œ ë³€í™˜
# -------------------------------------------------------

def normalize_gasnews_date(raw: str) -> str:
    raw = raw.strip()
    year = datetime.now().year
    try:
        return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d %H:%M").strftime("%Y-%m-%d")
    except:
        try:
            return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d").strftime("%Y-%m-%d")
        except:
            return datetime.now().strftime("%Y-%m-%d")


def normalize_electimes_date(raw: str) -> str:
    raw = raw.strip()
    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d"):
        try:
            return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
        except:
            continue
    return datetime.now().strftime("%Y-%m-%d")


# -------------------------------------------------------
# 4. íƒœê·¸ ìë™ ìƒì„±
# -------------------------------------------------------

def make_tags(title: str) -> list[str]:
    tags = [kw for kw in HYDROGEN_KEYWORDS if kw.lower() in title.lower()]
    return list(dict.fromkeys(tags))


# -------------------------------------------------------
# 5. ë¬¸ì¥ ë¶„ë¦¬ (lookbehind ì˜¤ë¥˜ í•´ê²° ë²„ì „)
# -------------------------------------------------------

def split_sentences(text: str) -> list[str]:
    """ë¬¸ì¥ ë¶„ë¦¬: lookbehind ì˜¤ë¥˜ ì—†ì´ í•œêµ­ì–´ ëŒ€ì‘."""
    if not text:
        return []

    cleaned = re.sub(r"\s+", " ", text).strip()

    # "ë‹¤." ë’¤ì—ì„œ ê°•ì œ ì¤„ë°”ê¿ˆ
    cleaned = cleaned.replace("ë‹¤. ", "ë‹¤.\n")
    cleaned = cleaned.replace("ë‹¤.", "ë‹¤.\n")

    # ì˜ì–´ê¶Œ ë¬¸ì¥ë¶€í˜¸ ê¸°ì¤€ìœ¼ë¡œ split (lookbehind ê³ ì •ê¸¸ì´)
    parts = re.split(r"(?<=[.!?])\s+", cleaned)

    sentences = []
    for p in parts:
        for seg in p.split("\n"):
            seg = seg.strip()
            if seg:
                sentences.append(seg)

    return sentences


# -------------------------------------------------------
# 6. ë³¸ë¬¸ ìš”ì•½ (3ì¤„ ìš”ì•½)
# -------------------------------------------------------

def summarize_body(body: str, max_lines: int = 3) -> str:
    if not body:
        return ""

    sents = split_sentences(body)
    if not sents:
        return ""

    return "\n".join(sents[:max_lines])


# -------------------------------------------------------
# 7. ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
# -------------------------------------------------------

def extract_article_body(url: str) -> str:
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
    except:
        return ""

    soup = BeautifulSoup(resp.text, "html.parser")

    # ê°€ìŠ¤ì‹ ë¬¸ / ì „ê¸°ì‹ ë¬¸ ëª¨ë‘ ê³µí†µì ìœ¼ë¡œ article ë³¸ë¬¸ div ì‚¬ìš©
    body_el = soup.select_one("div#article-view-content-div, div.article-body, div#articleBody")
    if not body_el:
        return ""

    texts = [x.get_text(" ", strip=True) for x in body_el.find_all(["p", "span", "div"])]
    body = " ".join(texts)
    return re.sub(r"\s+", " ", body).strip()


# -------------------------------------------------------
# 8. ê°€ìŠ¤ì‹ ë¬¸ í¬ë¡¤ëŸ¬
# -------------------------------------------------------

def crawl_gasnews(max_pages: int = 3) -> list[dict]:
    results = []

    for page in range(1, max_pages + 1):
        url = GASNEWS_LIST_URL.format(page=page)
        print(f"[ê°€ìŠ¤ì‹ ë¬¸] {page} í˜ì´ì§€ í¬ë¡¤ë§ â†’ {url}")

        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("section#section-list ul.type1 > li"):
            title_a = li.select_one("h4.titles a")
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            article_url = BASE_URL + title_a.get("href", "")

            date_el = li.select_one("em.info.dated")
            date_str = normalize_gasnews_date(date_el.get_text(strip=True))

            if not contains_hydrogen_keyword(title):
                continue

            body = extract_article_body(article_url)
            summary_3 = summarize_body(body, max_lines=3)

            results.append({
                "date": date_str,
                "source": "ê°€ìŠ¤ì‹ ë¬¸",
                "title": title,
                "url": article_url,
                "body": body,
                "summary": summary_3,
                "tags": make_tags(title)
            })

    return results


# -------------------------------------------------------
# 9. ì „ê¸°ì‹ ë¬¸ í¬ë¡¤ëŸ¬
# -------------------------------------------------------

def crawl_electimes(max_pages: int = 3) -> list[dict]:
    results = []

    for page in range(1, max_pages + 1):
        url = ELECTIMES_LIST_URL.format(page=page)
        print(f"[ì „ê¸°ì‹ ë¬¸] {page} í˜ì´ì§€ í¬ë¡¤ë§ â†’ {url}")

        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("#section-list ul.type > li.item"):
            title_a = li.select_one("h4.titles a.replace-titles")
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            article_url = ELECTIMES_BASE_URL + title_a.get("href", "")

            date_el = li.select_one("em.replace-date")
            date_str = normalize_electimes_date(date_el.get_text(strip=True))

            summary_el = li.select_one("p.lead a.replace-read")
            summary_text = summary_el.get_text(strip=True) if summary_el else ""

            if not contains_hydrogen_keyword(f"{title} {summary_text}"):
                continue

            body = extract_article_body(article_url)
            summary_3 = summarize_body(body, max_lines=3)

            results.append({
                "date": date_str,
                "source": "ì „ê¸°ì‹ ë¬¸",
                "title": title,
                "url": article_url,
                "body": body,
                "summary": summary_3,
                "tags": make_tags(title)
            })

    return results


# -------------------------------------------------------
# 10. ì¹´ë“œë‰´ìŠ¤ (JSON ì €ì¥)
# -------------------------------------------------------

from cardnews_image import make_cardnews_image

def main():
    today = datetime.now().strftime("%Y-%m-%d")
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    all_articles = []
    all_articles.extend(crawl_gasnews(max_pages=3))
    all_articles.extend(crawl_electimes(max_pages=3))

    today_articles = [a for a in all_articles if a["date"] == today]

    # -------------------------
    # ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ ìƒì„±
    # -------------------------
    for idx, article in enumerate(today_articles):
        # ì¹´ë“œë‰´ìŠ¤ìš© í…ìŠ¤íŠ¸ êµ¬ì„±
        card_text = f"{article['title']}\n\n{article['summary']}"
        image_filename = f"{today}_{idx+1}.png"
        image_path = data_dir / image_filename

        make_cardnews_image(card_text, image_path)

        article["image"] = image_filename  # JSONì— ê¸°ë¡

    # -------------------------
    # JSON ì €ì¥
    # -------------------------
    out_path = data_dir / f"{today}.json"
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(today_articles, f, ensure_ascii=False, indent=2)

    print(f"ì™„ë£Œ: {len(today_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ / ì¹´ë“œë‰´ìŠ¤ PNG ìƒì„± ì™„ë£Œ")




# -------------------------------------------------------
# 11. ë©”ì¸ (JSON ì €ì¥)
# -------------------------------------------------------

def main():
    today = datetime.now().strftime("%Y-%m-%d")

    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    all_articles = []
    all_articles.extend(crawl_gasnews(max_pages=3))
    all_articles.extend(crawl_electimes(max_pages=3))

    today_articles = [a for a in all_articles if a["date"] == today]

    out_path = data_dir / f"{today}.json"
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(today_articles, f, ensure_ascii=False, indent=2)

    print(f"ğŸŸ¢ ì €ì¥ ì™„ë£Œ: {len(today_articles)}ê±´ â†’ {out_path}")


if __name__ == "__main__":
    main()
