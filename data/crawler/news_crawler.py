import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
from pathlib import Path
import os
import re

# ==========================================
# 1. ì„¤ì •
# ==========================================

KEYWORDS = [
    "ìˆ˜ì†Œ", "ì—°ë£Œì „ì§€", "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ", "ì›ìë ¥",
    "PAFC", "SOFC", "MCFC", "PEM", "ì¬ìƒ", "ë°°ì¶œê¶Œ", "íˆíŠ¸íŒí”„", "ë„ì‹œê°€ìŠ¤", "êµ¬ì—­ì „ê¸°", "PPA",
    "ìˆ˜ì „í•´", "ì „í•´ì¡°", "PEMEC", "AEM", "ì•Œì¹´ë¼ì¸", "ë¶„ì‚°", "NDC", "í•‘í¬ìˆ˜ì†Œ",
    "ì•”ëª¨ë‹ˆì•„", "ì•”ëª¨ë‹ˆì•„í¬ë˜í‚¹", "CCU", "CCUS", "ê¸°í›„ë¶€", "ESS", "ë°°í„°ë¦¬",
    "ìˆ˜ì†Œìƒì‚°", "ìˆ˜ì†Œì €ì¥", "ì•¡í™”ìˆ˜ì†Œ",
    "ì¶©ì „ì†Œ", "ìˆ˜ì†Œë²„ìŠ¤", "ìˆ˜ì†Œì°¨", "ì¸í”„ë¼",
    "í•œìˆ˜ì›", "ë‘ì‚°í“¨ì–¼ì…€", "í•œí™”ì„íŒ©íŠ¸", "í˜„ëŒ€ì°¨",
    "HPS", "HPC", "REC", "RPS"
]

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
LATEST_JSON_PATH = DATA_DIR / "latest.json"


# ==========================================
# 2. ìœ í‹¸ í•¨ìˆ˜
# ==========================================

def get_soup(url: str):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        return BeautifulSoup(res.text, "html.parser")
    except Exception as e:
        print(f"[ERROR] {url} â†’ {e}")
        return None


def check_keywords(title: str):
    """ì œëª©ì— í¬í•¨ëœ í‚¤ì›Œë“œë¥¼ íƒœê·¸ë¡œ ë¦¬í„´"""
    lower = title.lower()
    return [kw for kw in KEYWORDS if kw.lower() in lower]


def normalize_date_common(raw: str):
    """
    ì—¬ëŸ¬ ì‹ ë¬¸ ê³µí†µ ë‚ ì§œ íŒŒì„œ
    '2025.12.10', '2025.12.10 09:30', '2025-12-10', '12.10 09:30' ë“± ëŒ€ì‘
    """
    if not raw:
        return datetime.now().strftime("%Y-%m-%d")

    raw = raw.strip()

    # ì—°ë„ê¹Œì§€ ìˆëŠ” ê²½ìš°
    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d", "%Y-%m-%d"):
        try:
            return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
        except ValueError:
            pass

    # ì—°ë„ê°€ ì—†ëŠ” ì¼€ì´ìŠ¤
    year = datetime.now().year
    try:
        return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d %H:%M").strftime("%Y-%m-%d")
    except Exception:
        try:
            return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d").strftime("%Y-%m-%d")
        except Exception:
            return datetime.now().strftime("%Y-%m-%d")


# ==========================================
# 3. ë³¸ë¬¸ ì¶”ì¶œ & ìš”ì•½ (1 / 2 / 3ì¤„)
# ==========================================

def extract_article_body(url: str) -> str:
    """ê¸°ì‚¬ ìƒì„¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (3ê°œ ì‹ ë¬¸ ê³µí†µ ëŒ€ì‘)"""
    soup = get_soup(url)
    if not soup:
        return ""

    body_el = soup.select_one(
        "div#article-view-content-div, "
        "div.article-body, "
        "div#articleBody, "
        "div.article-text"
    )
    if not body_el:
        texts = [p.get_text(" ", strip=True) for p in soup.select("p")]
    else:
        texts = [x.get_text(" ", strip=True) for x in body_el.find_all(["p", "span", "div"])]

    body = " ".join(texts)
    body = re.sub(r"\s+", " ", body).strip()
    return body


def split_sentences(text: str):
    """lookbehind ë¬¸ì œ ì—†ëŠ” í•œêµ­ì–´ + ì˜ì–´ í˜¼í•© ë¬¸ì¥ ë¶„ë¦¬"""
    if not text:
        return []

    cleaned = re.sub(r"\s+", " ", text).strip()

    # 'ë‹¤.' ê¸°ì¤€ìœ¼ë¡œ ì¤„ë°”ê¿ˆ
    cleaned = cleaned.replace("ë‹¤. ", "ë‹¤.\n")
    cleaned = cleaned.replace("ë‹¤.", "ë‹¤.\n")

    # ì˜ì–´ê¶Œ ë¬¸ì¥ë¶€í˜¸ ê¸°ì¤€ ë¶„ë¦¬
    parts = re.split(r"(?<=[.!?])\s+", cleaned)

    sentences = []
    for p in parts:
        for seg in p.split("\n"):
            seg = seg.strip()
            if seg:
                sentences.append(seg)

    return sentences


def summarize_variants(body: str):
    """
    ë³¸ë¬¸ì—ì„œ
      - 1ì¤„ ìš”ì•½: ì²« ë¬¸ì¥
      - 2ì¤„ ìš”ì•½: ì• 2ë¬¸ì¥
      - 3ì¤„ ìš”ì•½: ì• 3ë¬¸ì¥
    ì„ ë§Œë“¤ì–´ì„œ dictë¡œ ë°˜í™˜
    """
    sents = split_sentences(body)
    if not sents:
        return {"one": "", "two": "", "three": ""}

    one = sents[0]
    two = " ".join(sents[:2]) if len(sents) >= 2 else one
    three = " ".join(sents[:3]) if len(sents) >= 3 else two

    # index.htmlì—ì„œ í•œ ì¤„ë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ê°œí–‰ ì œê±°
    return {
        "one": one.replace("\n", " "),
        "two": two.replace("\n", " "),
        "three": three.replace("\n", " "),
    }


# ==========================================
# 4. ê° ì‹ ë¬¸ë³„ í¬ë¡¤ëŸ¬
#    ğŸ‘‰ ìˆ˜ì • í¬ì¸íŠ¸: tagsê°€ ë¹„ì–´ ìˆìœ¼ë©´ ê·¸ ê¸°ì‚¬ëŠ” ë²„ë¦¼
# ==========================================

def crawl_energy_news():
    """ì—ë„ˆì§€ì‹ ë¬¸"""
    print("   [ì—ë„ˆì§€ì‹ ë¬¸] í¬ë¡¤ë§ ì‹œì‘...")
    results = []
    base_url = "https://www.energy-news.co.kr"
    url = f"{base_url}/news/articleList.html?view_type=sm"

    soup = get_soup(url)
    if not soup:
        return results

    articles = soup.select("#section-list .type1 li")
    for art in articles:
        try:
            title_tag = art.select_one("h2.titles a")
            if not title_tag:
                continue

            title = title_tag.get_text(strip=True)
            link = title_tag["href"]
            if not link.startswith("http"):
                link = base_url + link

            # ğŸ”¹ ì œëª©ì—ì„œ í‚¤ì›Œë“œ íƒœê·¸ ìƒì„±
            tags = check_keywords(title)
            # ğŸ”¹ ìˆ˜ì†Œ ê´€ë ¨ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
            if not tags:
                continue

            date_tag = art.select_one("em.info.dated")
            raw_date = date_tag.get_text(strip=True) if date_tag else ""
            date = normalize_date_common(raw_date)

            body = extract_article_body(link)
            summaries = summarize_variants(body)

            results.append({
                "source": "ì—ë„ˆì§€ì‹ ë¬¸",
                "title": title,
                "url": link,
                "date": date,
                "tags": tags,                  # ìµœì†Œ 1ê°œ ì´ìƒ ë³´ì¥
                "summary1": summaries["one"],  # 1ì¤„ ìš”ì•½
                "subtitle": summaries["two"],  # 2ì¤„ ìš”ì•½(ê¸°ë³¸)
                "summary3": summaries["three"],# 3ì¤„ ìš”ì•½
                "is_important": True,          # íƒœê·¸ê°€ ìˆìœ¼ë¯€ë¡œ True
            })
        except Exception:
            continue

    return results


def crawl_gas_news():
    """ê°€ìŠ¤ì‹ ë¬¸"""
    print("   [ê°€ìŠ¤ì‹ ë¬¸] í¬ë¡¤ë§ ì‹œì‘...")
    results = []
    base_url = "https://www.gasnews.com"
    url = f"{base_url}/news/articleList.html?view_type=sm"

    soup = get_soup(url)
    if not soup:
        return results

    articles = soup.select("#section-list .type1 li")
    if not articles:
        articles = soup.select(".article-list .list-block")

    for art in articles:
        try:
            title_tag = art.select_one("h2.titles a") or art.select_one("h4.titles a")
            if not title_tag:
                continue

            title = title_tag.get_text(strip=True)
            link = title_tag["href"]
            if not link.startswith("http"):
                link = base_url + link

            # ğŸ”¹ í‚¤ì›Œë“œ ì²´í¬
            tags = check_keywords(title)
            if not tags:
                continue

            date_tag = art.select_one("em.info.dated")
            raw_date = date_tag.get_text(strip=True) if date_tag else ""
            date = normalize_date_common(raw_date)

            body = extract_article_body(link)
            summaries = summarize_variants(body)

            results.append({
                "source": "ê°€ìŠ¤ì‹ ë¬¸",
                "title": title,
                "url": link,
                "date": date,
                "tags": tags,
                "summary1": summaries["one"],
                "subtitle": summaries["two"],
                "summary3": summaries["three"],
                "is_important": True,
            })
        except Exception:
            continue

    return results


def crawl_electric_news():
    """ì „ê¸°ì‹ ë¬¸"""
    print("   [ì „ê¸°ì‹ ë¬¸] í¬ë¡¤ë§ ì‹œì‘...")
    results = []
    base_url = "https://www.electimes.com"
    url = f"{base_url}/news/articleList.html?view_type=sm"

    soup = get_soup(url)
    if not soup:
        return results

    articles = soup.select("#section-list .type1 li")
    for art in articles:
        try:
            title_tag = art.select_one("h2.titles a") or art.select_one("h4.titles a")
            if not title_tag:
                continue

            title = title_tag.get_text(strip=True)
            link = title_tag["href"]
            if not link.startswith("http"):
                link = base_url + link

            # ğŸ”¹ í‚¤ì›Œë“œ ì²´í¬
            tags = check_keywords(title)
            if not tags:
                continue

            date_tag = art.select_one("em.info.dated")
            raw_date = date_tag.get_text(strip=True) if date_tag else ""
            date = normalize_date_common(raw_date)

            body = extract_article_body(link)
            summaries = summarize_variants(body)

            results.append({
                "source": "ì „ê¸°ì‹ ë¬¸",
                "title": title,
                "url": link,
                "date": date,
                "tags": tags,
                "summary1": summaries["one"],
                "subtitle": summaries["two"],
                "summary3": summaries["three"],
                "is_important": True,
            })
        except Exception:
            continue

    return results


# ==========================================
# 5. í†µí•© ì‹¤í–‰ + latest.json ì €ì¥
# ==========================================

def job():
    print(f"\n[í¬ë¡¤ë§ ì‹œì‘] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    all_data = []
    all_data.extend(crawl_energy_news())
    all_data.extend(crawl_gas_news())
    all_data.extend(crawl_electric_news())

    # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    dedup = {}
    for art in all_data:
        dedup[art["url"]] = art
    unique_articles = list(dedup.values())

    # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ë§Œ ë‚¨ê¸°ê¸°
    today = datetime.now().strftime("%Y-%m-%d")
    unique_articles = [a for a in unique_articles if a["date"] == today]

    # ì´ë¯¸ is_important = Trueë¡œ ê³ ì •ì´ì§€ë§Œ, í˜¹ì‹œ í™•ì¥ ëŒ€ë¹„ ì •ë ¬ ìœ ì§€
    unique_articles.sort(key=lambda x: x["is_important"], reverse=True)

    with LATEST_JSON_PATH.open("w", encoding="utf-8") as f:
        json.dump(unique_articles, f, ensure_ascii=False, indent=2)

    print(f"[ì™„ë£Œ] {len(unique_articles)}ê±´ ìˆ˜ì§‘ â†’ {LATEST_JSON_PATH}")


# ==========================================
# 6. ë©”ì¸
# ==========================================

if __name__ == "__main__":
    job()
