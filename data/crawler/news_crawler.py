import json
import re
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup

from cardnews_image import make_cardnews_image

# -------------------------------------------------------
# 1. ê¸°ë³¸ ì„¤ì •
# -------------------------------------------------------

BASE_URL = "https://www.gasnews.com"

GASNEWS_LIST_URL = (
    "https://www.gasnews.com/news/articleList.html?"
    "page={page}&sc_section_code=S1N9&view_type="
)

ELECTIMES_BASE_URL = "https://www.electimes.com"
ELECTIMES_LIST_URL = (
    "https://www.electimes.com/news/articleList.html?page={page}&view_type=sm"
)

# -------------------------------------------------------
# 2. ìˆ˜ì†Œ í‚¤ì›Œë“œ (ëª¨ë“  ì‹ ë¬¸ ê³µí†µ)
# -------------------------------------------------------

HYDROGEN_KEYWORDS = [
    "ìˆ˜ì†Œ", "ì—°ë£Œì „ì§€", "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ", "ì›ìë ¥",
    "PAFC", "SOFC", "MCFC", "PEM", "ì¬ìƒ", "ë°°ì¶œê¶Œ", "íˆíŠ¸íŒí”„", "ë„ì‹œê°€ìŠ¤", "êµ¬ì—­ì „ê¸°", "PPA",
    "ìˆ˜ì „í•´", "ì „í•´ì¡°", "PEMEC", "AEM", "ì•Œì¹´ë¼ì¸", "ë¶„ì‚°", "NDC", "í•‘í¬ìˆ˜ì†Œ",
    "ì•”ëª¨ë‹ˆì•„", "ì•”ëª¨ë‹ˆì•„í¬ë˜í‚¹", "CCU", "CCUS", "ê¸°í›„ë¶€", "ESS", "ë°°í„°ë¦¬",
    "ìˆ˜ì†Œìƒì‚°", "ìˆ˜ì†Œì €ì¥", "ì•¡í™”ìˆ˜ì†Œ",
    "ì¶©ì „ì†Œ", "ìˆ˜ì†Œë²„ìŠ¤", "ìˆ˜ì†Œì°¨", "ì¸í”„ë¼",
    "í•œìˆ˜ì›", "ë‘ì‚°í“¨ì–¼ì…€", "í•œí™”ì„íŒ©íŠ¸", "í˜„ëŒ€ì°¨",
    "HPS", "HPC", "REC", "RPS",
]


def contains_hydrogen_keyword(text: str) -> bool:
    text = (text or "").lower()
    return any(kw.lower() in text for kw in HYDROGEN_KEYWORDS)


# -------------------------------------------------------
# 3. ë‚ ì§œ ë³€í™˜
# -------------------------------------------------------

def normalize_gasnews_date(raw: str) -> str:
    raw = (raw or "").strip()
    year = datetime.now().year
    try:
        return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d %H:%M").strftime("%Y-%m-%d")
    except Exception:
        try:
            return datetime.strptime(f"{year}.{raw}", "%Y.%m.%d").strftime("%Y-%m-%d")
        except Exception:
            return datetime.now().strftime("%Y-%m-%d")


def normalize_electimes_date(raw: str) -> str:
    raw = (raw or "").strip()
    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d"):
        try:
            return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
        except Exception:
            continue
    return datetime.now().strftime("%Y-%m-%d")


# -------------------------------------------------------
# 4. íƒœê·¸ ìë™ ìƒì„±
# -------------------------------------------------------

def make_tags(title: str) -> list[str]:
    title = title or ""
    tags = [kw for kw in HYDROGEN_KEYWORDS if kw.lower() in title.lower()]
    return list(dict.fromkeys(tags))


# -------------------------------------------------------
# 5. ë¬¸ì¥ ë¶„ë¦¬ (lookbehind ì˜¤ë¥˜ ì—†ëŠ” ë²„ì „)
# -------------------------------------------------------

def split_sentences(text: str) -> list[str]:
    if not text:
        return []

    cleaned = re.sub(r"\s+", " ", text).strip()

    # "ë‹¤." ë’¤ì—ì„œ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ (ì•„ì£¼ ë‹¨ìˆœí•œ í•œêµ­ì–´ìš© heuristic)
    cleaned = cleaned.replace("ë‹¤. ", "ë‹¤.\n")
    cleaned = cleaned.replace("ë‹¤.", "ë‹¤.\n")

    # ì˜ì–´ì‹ ë¬¸ì¥ë¶€í˜¸ ê¸°ì¤€ ë¶„í•  (lookbehind ê³ ì • ê¸¸ì´ë§Œ ì‚¬ìš©)
    parts = re.split(r"(?<=[.!?])\s+", cleaned)

    sentences = []
    for p in parts:
        for seg in p.split("\n"):
            seg = seg.strip()
            if seg:
                sentences.append(seg)

    return sentences


# -------------------------------------------------------
# 6. ë³¸ë¬¸ 3ì¤„ ìš”ì•½
# -------------------------------------------------------

def summarize_body(body: str, max_lines: int = 3) -> str:
    if not body:
        return ""
    sents = split_sentences(body)
    if not sents:
        return ""
    return "\n".join(sents[:max_lines])


# -------------------------------------------------------
# 7. ìƒì„¸ ë³¸ë¬¸ í¬ë¡¤ë§
# -------------------------------------------------------

def extract_article_body(url: str) -> str:
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"[WARN] ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {url} ({e})")
        return ""

    soup = BeautifulSoup(resp.text, "html.parser")

    body_el = soup.select_one(
        "div#article-view-content-div, div.article-body, div#articleBody"
    )
    if not body_el:
        return ""

    texts = [
        x.get_text(" ", strip=True)
        for x in body_el.find_all(["p", "span", "div"])
    ]
    body = " ".join(texts)
    return re.sub(r"\s+", " ", body).strip()


# -------------------------------------------------------
# 8. ê°€ìŠ¤ì‹ ë¬¸ í¬ë¡¤ëŸ¬
# -------------------------------------------------------

def crawl_gasnews(max_pages: int = 3) -> list[dict]:
    results: list[dict] = []

    for page in range(1, max_pages + 1):
        url = GASNEWS_LIST_URL.format(page=page)
        print(f"[ê°€ìŠ¤ì‹ ë¬¸] {page} í˜ì´ì§€ í¬ë¡¤ë§ â†’ {url}")

        try:
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            print(f"[WARN] ê°€ìŠ¤ì‹ ë¬¸ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨(page={page}): {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("section#section-list ul.type1 > li"):
            try:
                title_a = li.select_one("h4.titles a")
                if not title_a:
                    continue

                title = title_a.get_text(strip=True)
                href = title_a.get("href") or ""
                article_url = href if href.startswith("http") else BASE_URL + href

                date_el = li.select_one("em.info.dated")
                date_str = normalize_gasnews_date(
                    date_el.get_text(strip=True) if date_el else ""
                )

                # ì œëª© ê¸°ë°˜ í•„í„°
                if not contains_hydrogen_keyword(title):
                    continue

                body = extract_article_body(article_url)
                summary_3 = summarize_body(body, max_lines=3)

                results.append(
                    {
                        "date": date_str,
                        "source": "ê°€ìŠ¤ì‹ ë¬¸",
                        "title": title,
                        "url": article_url,
                        "body": body,
                        "summary": summary_3,
                        "tags": make_tags(title),
                    }
                )
            except Exception as e:
                print(f"[WARN] ê°€ìŠ¤ì‹ ë¬¸ ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue

    return results


# -------------------------------------------------------
# 9. ì „ê¸°ì‹ ë¬¸ í¬ë¡¤ëŸ¬
# -------------------------------------------------------

def crawl_electimes(max_pages: int = 3) -> list[dict]:
    results: list[dict] = []

    for page in range(1, max_pages + 1):
        url = ELECTIMES_LIST_URL.format(page=page)
        print(f"[ì „ê¸°ì‹ ë¬¸] {page} í˜ì´ì§€ í¬ë¡¤ë§ â†’ {url}")

        try:
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            print(f"[WARN] ì „ê¸°ì‹ ë¬¸ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨(page={page}): {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("#section-list ul.type > li.item"):
            try:
                title_a = li.select_one("div.view-cont h4.titles a.replace-titles")
                if not title_a:
                    continue

                title = title_a.get_text(strip=True)
                href = title_a.get("href") or ""
                article_url = (
                    href if href.startswith("http") else ELECTIMES_BASE_URL + href
                )

                date_el = li.select_one("div.view-cont em.replace-date")
                date_str = normalize_electimes_date(
                    date_el.get_text(strip=True) if date_el else ""
                )

                summary_el = li.select_one("div.view-cont p.lead a.replace-read")
                summary_text = (
                    summary_el.get_text(strip=True) if summary_el else ""
                )

                # ì œëª© + ë¦¬ë“œë¬¸ ê¸°ë°˜ í•„í„°
                if not contains_hydrogen_keyword(f"{title} {summary_text}"):
                    continue

                body = extract_article_body(article_url)
                summary_3 = summarize_body(body, max_lines=3)

                results.append(
                    {
                        "date": date_str,
                        "source": "ì „ê¸°ì‹ ë¬¸",
                        "title": title,
                        "url": article_url,
                        "body": body,
                        "summary": summary_3,
                        "tags": make_tags(title),
                    }
                )
            except Exception as e:
                print(f"[WARN] ì „ê¸°ì‹ ë¬¸ ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue

    return results


# -------------------------------------------------------
# 10. ë©”ì¸: ì˜¤ëŠ˜ ê¸°ì‚¬ + ì¹´ë“œë‰´ìŠ¤ PNG + JSON ì €ì¥
# -------------------------------------------------------

def main():
    today = datetime.now().strftime("%Y-%m-%d")

    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    all_articles: list[dict] = []
    all_articles.extend(crawl_gasnews(max_pages=3))
    all_articles.extend(crawl_electimes(max_pages=3))

    # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ë§Œ í•„í„°
    today_articles = [a for a in all_articles if a.get("date") == today]

    # ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ ìƒì„±
    for idx, article in enumerate(today_articles, start=1):
        card_text = f"{article['title']}\n\n{article['summary']}"
        image_filename = f"{today}_{idx}.png"
        image_path = data_dir / image_filename

        make_cardnews_image(card_text, image_path)
        article["image"] = image_filename

    # JSON ì €ì¥
    out_path = data_dir / f"{today}.json"
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(today_articles, f, ensure_ascii=False, indent=2)

    print(f"\nğŸŸ¢ ì™„ë£Œ: {len(today_articles)}ê±´ â†’ {out_path}")
    print("ğŸŸ¢ ì¹´ë“œë‰´ìŠ¤ PNGë„ data/ í´ë”ì— ìƒì„±ë¨")


if __name__ == "__main__":
    main()
