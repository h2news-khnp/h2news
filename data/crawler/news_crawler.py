import json
import re
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup

# ==========================================
# 1. ì„¤ì •
# ==========================================

KEYWORDS = [
    "ìˆ˜ì†Œ", "ì—°ë£Œì „ì§€", "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ", "ì›ìë ¥",
    "PAFC", "SOFC", "MCFC", "PEM", "ì¬ìƒ", "ë°°ì¶œê¶Œ", "íˆíŠ¸íŒí”„", "ë„ì‹œê°€ìŠ¤", "êµ¬ì—­ì „ê¸°", "PPA",
    "ìˆ˜ì „í•´", "ì „í•´ì¡°", "PEMEC", "AEM", "ì•Œì¹´ë¼ì¸", "ë¶„ì‚°", "NDC", "í•‘í¬ìˆ˜ì†Œ",
    "ì•”ëª¨ë‹ˆì•„", "ì•”ëª¨ë‹ˆì•„í¬ë˜í‚¹", "CCU", "CCUS", "ê¸°í›„ë¶€", "ESS", "ë°°í„°ë¦¬",
    "ìˆ˜ì†Œìƒì‚°", "ìˆ˜ì†Œì €ì¥", "ì•¡í™”ìˆ˜ì†Œ",
    "ì¶©ì „ì†Œ", "ìˆ˜ì†Œë²„ìŠ¤", "ìˆ˜ì†Œì°¨", "ì¸í”„ë¼",
    "í•œìˆ˜ì›", "ë‘ì‚°í“¨ì–¼ì…€", "í•œí™”ì„íŒ©íŠ¸", "í˜„ëŒ€ì°¨",
    "HPS", "HPC", "REC", "RPS"
]

MAX_PAGES = 3
TIMEOUT = 12

DATA_DIR = Path("data")
BY_DATE_DIR = DATA_DIR / "by_date"
DATA_DIR.mkdir(exist_ok=True)
BY_DATE_DIR.mkdir(exist_ok=True)

ALL_JSON_PATH = DATA_DIR / "all.json"
LATEST_JSON_PATH = DATA_DIR / "latest.json"

ENERGY_BASE = "https://www.energy-news.co.kr"
GAS_BASE = "https://www.gasnews.com"
ELECT_BASE = "https://www.electimes.com"

ENERGY_LIST = ENERGY_BASE + "/news/articleList.html?page={page}&view_type=sm"
GAS_LIST = GAS_BASE + "/news/articleList.html?page={page}&view_type=sm"
ELECT_LIST = ELECT_BASE + "/news/articleList.html?page={page}&view_type=sm"

# ==========================================
# 2. ê³µí†µ ìœ í‹¸
# ==========================================

def now_str():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def normalize_spaces(text: str) -> str:
    return re.sub(r"\s+", " ", (text or "")).strip()

def get_soup(url: str):
    try:
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=TIMEOUT)
        r.raise_for_status()
        return BeautifulSoup(r.text, "html.parser")
    except Exception as e:
        print(f"[ERROR] {url} â†’ {e}")
        return None

def parse_date(raw: str) -> str:
    raw = (raw or "").strip()
    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d", "%Y-%m-%d"):
        try:
            return datetime.strptime(raw, fmt).strftime("%Y-%m-%d")
        except ValueError:
            pass

    year = datetime.now().year
    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d"):
        try:
            return datetime.strptime(f"{year}.{raw}", fmt).strftime("%Y-%m-%d")
        except ValueError:
            pass

    return datetime.now().strftime("%Y-%m-%d")

def contains_keyword(text: str) -> bool:
    low = (text or "").lower()
    return any(k.lower() in low for k in KEYWORDS)

def make_tags(text: str) -> list:
    low = (text or "").lower()
    seen = set()
    tags = []
    for k in KEYWORDS:
        if k.lower() in low and k not in seen:
            tags.append(k)
            seen.add(k)
    return tags

# ==========================================
# 3. ë³¸ë¬¸ ì •ì œ (ğŸ”¥ ì „ê¸°ì‹ ë¬¸ í•µì‹¬ ìˆ˜ì •)
# ==========================================

def clean_article_body(text: str) -> str:
    if not text:
        return ""

    remove_patterns = [
        r"[ê°€-í£]{2,4}\sê¸°ì",
        r"\([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+\)",
        r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+",
        r"ì œë³´.*",
        r"í˜ì´ìŠ¤ë¶.*",
        r"íŠ¸ìœ„í„°.*",
        r"ì¹´ì¹´ì˜¤.*",
        r"SNS.*",
        r"ê³µìœ .*",
        r"ê¸°ì‚¬ë³´ë‚´ê¸°.*"
    ]

    cleaned = text
    for p in remove_patterns:
        cleaned = re.sub(p, "", cleaned)

    return normalize_spaces(cleaned)

def split_sentences_ko(text: str) -> list:
    text = normalize_spaces(text)
    text = text.replace("ë‹¤. ", "ë‹¤.\n").replace("ë‹¤.", "ë‹¤.\n")
    parts = re.split(r"(?<=[.!?])\s+", text)

    out = []
    for p in parts:
        for seg in p.split("\n"):
            seg = seg.strip()
            if seg:
                out.append(seg)
    return out

def summarize_2lines(body: str) -> str:
    sents = split_sentences_ko(body)
    return normalize_spaces(" ".join(sents[:2])) if sents else ""

# ==========================================
# 4. ë³¸ë¬¸ ì¶”ì¶œ
# ==========================================

def extract_body(url: str) -> str:
    soup = get_soup(url)
    if not soup:
        return ""

    selectors = [
        "div#article-view-content-div",
        "div#articleBody",
        "div.article-body",
        "div.article-text",
        "article"
    ]

    body_el = None
    for sel in selectors:
        body_el = soup.select_one(sel)
        if body_el:
            break

    texts = []
    if body_el:
        for t in body_el.find_all(["p", "span", "div"]):
            txt = t.get_text(" ", strip=True)
            if txt:
                texts.append(txt)
    else:
        for p in soup.select("p"):
            txt = p.get_text(" ", strip=True)
            if txt:
                texts.append(txt)

    body = normalize_spaces(" ".join(texts))
    body = clean_article_body(body)

    return body if len(body) >= 40 else ""

# ==========================================
# 5. ëª©ë¡ í¬ë¡¤ëŸ¬ (1~3í˜ì´ì§€)
# ==========================================

def crawl_list(list_url, base_url, source):
    results = []

    for page in range(1, MAX_PAGES + 1):
        soup = get_soup(list_url.format(page=page))
        if not soup:
            continue

        items = soup.select("#section-list li")
        kept = 0

        for li in items:
            try:
                a = li.select_one("h2.titles a, h4.titles a, a.replace-titles")
                if not a:
                    continue

                title = a.get_text(strip=True)
                href = a.get("href", "")
                url = href if href.startswith("http") else base_url + href

                date_el = li.select_one("em.info.dated")
                date = parse_date(date_el.get_text(strip=True) if date_el else "")

                body = extract_body(url)

                if not (contains_keyword(title) or contains_keyword(body)):
                    continue

                tags = make_tags(title + " " + body)
                subtitle = summarize_2lines(body)

                results.append({
                    "source": source,
                    "title": title,
                    "url": url,
                    "date": date,
                    "tags": tags,
                    "subtitle": subtitle,
                    "is_important": 1 if tags else 0
                })
                kept += 1
            except Exception:
                continue

        print(f"[{source}] page {page} â†’ {kept}ê±´")

    return results

# ==========================================
# 6. ì €ì¥ ë¡œì§
# ==========================================

def job():
    print(f"\n[í¬ë¡¤ë§ ì‹œì‘] {now_str()}")

    new_items = []
    new_items += crawl_list(ENERGY_LIST, ENERGY_BASE, "ì—ë„ˆì§€ì‹ ë¬¸")
    new_items += crawl_list(GAS_LIST, GAS_BASE, "ê°€ìŠ¤ì‹ ë¬¸")
    new_items += crawl_list(ELECT_LIST, ELECT_BASE, "ì „ê¸°ì‹ ë¬¸")

    # ëˆ„ì  ë³‘í•©
    existing = json.loads(ALL_JSON_PATH.read_text("utf-8")) if ALL_JSON_PATH.exists() else []
    merged = {i["url"]: i for i in existing + new_items}.values()
    merged = sorted(merged, key=lambda x: (x["date"], x["is_important"]), reverse=True)

    ALL_JSON_PATH.write_text(json.dumps(list(merged), ensure_ascii=False, indent=2), encoding="utf-8")

    # ë‚ ì§œë³„ ì €ì¥
    by_date = {}
    for i in merged:
        by_date.setdefault(i["date"], []).append(i)

    for d, lst in by_date.items():
        (BY_DATE_DIR / f"{d}.json").write_text(
            json.dumps(lst, ensure_ascii=False, indent=2), encoding="utf-8"
        )

    latest_date = max(by_date.keys())
    LATEST_JSON_PATH.write_text(
        json.dumps(by_date[latest_date], ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    print(f"[ì™„ë£Œ] ì´ {len(merged)}ê±´ | ìµœì‹ ë‚ ì§œ {latest_date}")

if __name__ == "__main__":
    job()
