import json
import re
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup

# ============================================
# 1. ê¸°ë³¸ ì„¤ì •
# ============================================

# ê°€ìŠ¤ì‹ ë¬¸
GAS_BASE_URL = "https://www.gasnews.com"
GAS_LIST_URL = (
    "https://www.gasnews.com/news/articleList.html?"
    "page={page}&sc_section_code=S1N9&view_type="
)

# ì „ê¸°ì‹ ë¬¸
ELECT_BASE_URL = "https://www.electimes.com"
ELECT_LIST_URL = (
    "https://www.electimes.com/news/articleList.html?page={page}&view_type=sm"
)

# ì €ì¥ ê²½ë¡œ (ì´ íŒŒì¼: data/crawler/news_crawler.py ê¸°ì¤€)
SCRIPT_DIR = Path(__file__).resolve().parent          # .../data/crawler
DATA_DIR = SCRIPT_DIR.parent                          # .../data
CARDS_ROOT = DATA_DIR / "cards"                       # ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ ë£¨íŠ¸


# ============================================
# 2. ê³µí†µ: ìˆ˜ì†Œ ê´€ë ¨ í‚¤ì›Œë“œ
# ============================================

HYDROGEN_KEYWORDS = [
    # ê¸°ë³¸
    "ìˆ˜ì†Œ", "ì—°ë£Œì „ì§€", "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ",
    "PAFC", "SOFC", "MCFC",

    # ìˆ˜ì „í•´/ì „í•´ì¡°
    "ìˆ˜ì „í•´", "ì „í•´ì¡°", "PEMEC", "AEM", "ì•Œì¹´ë¼ì¸",

    # ì•”ëª¨ë‹ˆì•„ ê¸°ë°˜
    "ì•”ëª¨ë‹ˆì•„", "ì•”ëª¨ë‹ˆì•„í¬ë˜í‚¹",

    # ì¸í”„ë¼ & ì •ì±…
    "ìˆ˜ì†Œìƒì‚°", "ìˆ˜ì†Œì €ì¥", "ì•¡í™”ìˆ˜ì†Œ",
    "ì¶©ì „ì†Œ", "ìˆ˜ì†Œë²„ìŠ¤", "ìˆ˜ì†Œì°¨", "ì¸í”„ë¼",

    # ê¸°ê´€/ê¸°ì—… í‚¤ì›Œë“œ
    "í•œìˆ˜ì›", "ë‘ì‚°í“¨ì–¼ì…€", "í•œí™”ì„íŒ©íŠ¸", "í˜„ëŒ€ì°¨",

    # ê¸°íƒ€
    "HPS", "HPC", "REC", "RPS",
]


def contains_hydrogen_keyword(text: str) -> bool:
    """ìˆ˜ì†Œ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€."""
    if not text:
        return False
    lower = text.lower()
    return any(k.lower() in lower for k in HYDROGEN_KEYWORDS)


def make_tags(title: str, body: str = "") -> list[str]:
    """ì œëª©/ë³¸ë¬¸ì—ì„œ íƒœê·¸ ì¶”ì¶œ."""
    base = (title or "") + " " + (body or "")
    tags = [kw for kw in HYDROGEN_KEYWORDS if kw.lower() in base.lower()]
    # ì¤‘ë³µ ì œê±°
    return list(dict.fromkeys(tags))


# ============================================
# 3. ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
# ============================================

def normalize_gas_date(raw: str) -> str:
    """
    ê°€ìŠ¤ì‹ ë¬¸: '12.09 09:50'  -> 'YYYY-12-09'
    """
    raw = (raw or "").strip()
    year = datetime.now().year

    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d"):
        try:
            dt = datetime.strptime(f"{year}.{raw}", fmt)
            return dt.strftime("%Y-%m-%d")
        except ValueError:
            continue

    return datetime.now().strftime("%Y-%m-%d")


def normalize_elect_date(raw: str) -> str:
    raw = (raw or "").strip()
    for fmt in ("%Y.%m.%d %H:%M", "%Y.%m.%d"):
        try:
            dt = datetime.strptime(raw, fmt)
            return dt.strftime("%Y-%m-%d")
        except ValueError:
            continue
    return datetime.now().strftime("%Y-%m-%d")


# ============================================
# 4. ìƒì„¸ ë³¸ë¬¸ ì¶”ì¶œ & ìš”ì•½
# ============================================

def extract_article_body(url: str) -> str:
    """
    ìƒì„¸ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ.
    (ê°€ìŠ¤ì‹ ë¬¸/ì „ê¸°ì‹ ë¬¸ ëª¨ë‘ ê³µí†µ íŒ¨í„´ ìš°ì„  ì‹œë„)
    """
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        print(f"[WARN] ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {url} ({e})")
        return ""

    soup = BeautifulSoup(resp.text, "html.parser")

    # 1) ê°€ì¥ í”í•œ íŒ¨í„´ ì‹œë„
    candidates = [
        "#article-view-content-div",
        "section#article-view-content-div",
        "div#article-view-content-div",
        "div.article-body",
        "div#article-view-content-attach",
    ]

    for sel in candidates:
        el = soup.select_one(sel)
        if el:
            text = el.get_text(" ", strip=True)
            if len(text) > 30:
                return text

    # 2) fallback: article íƒœê·¸ ì „ì²´ ì‚¬ìš©
    article_el = soup.find("article")
    if article_el:
        text = article_el.get_text(" ", strip=True)
        if len(text) > 30:
            return text

    # 3) ìµœì¢… fallback: í˜ì´ì§€ ì „ì²´ì—ì„œ p íƒœê·¸ ëª¨ìŒ
    ps = soup.find_all("p")
    joined = " ".join(p.get_text(" ", strip=True) for p in ps)
    return joined.strip()


def split_sentences(text: str) -> list[str]:
    """
    ì•„ì£¼ ë‹¨ìˆœí•œ í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ë¶„ë¦¬.
    ì •êµí•˜ì§„ ì•Šì§€ë§Œ ì¹´ë“œë‰´ìŠ¤ 3ì¤„ ìš”ì•½ìš©ìœ¼ë¡œëŠ” ì¶©ë¶„.
    """
    if not text:
        return []

    # ì¤„ë°”ê¿ˆ â†’ ê³µë°±
    cleaned = re.sub(r"\s+", " ", text)

    # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/â€˜ë‹¤.â€™ ë’¤ì—ì„œ ë¶„ë¦¬
    parts = re.split(r"(?<=[\.!?]|ë‹¤\.)\s+", cleaned)
    sentences = [p.strip() for p in parts if p.strip()]
    return sentences


def summarize_body(body: str, max_lines: int = 3) -> str:
    """
    ë³¸ë¬¸ì—ì„œ ì•ìª½ ë¬¸ì¥ ìœ„ì£¼ë¡œ 3ì¤„ ìš”ì•½.
    (ì¤„ ì‚¬ì´ì— '\n' ì‚½ì…)
    """
    sents = split_sentences(body)
    if not sents:
        return ""

    picked = sents[:max_lines]

    # ë¬¸ì¥ ê°œìˆ˜ê°€ ëª¨ìë¼ë©´, ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ì˜ë¼ì„œ ê¸¸ì´ ì¡°ì •
    if len(picked) < max_lines and len(sents) > max_lines:
        extra = " ".join(sents[max_lines:])
        if extra:
            picked.append(extra[:80] + "...")
    return "\n".join(picked)


# ============================================
# 5. í¬ë¡¤ëŸ¬: ê°€ìŠ¤ì‹ ë¬¸
# ============================================

def crawl_gasnews(max_pages: int = 2) -> list[dict]:
    results: list[dict] = []

    for page in range(1, max_pages + 1):
        url = GAS_LIST_URL.format(page=page)
        print(f"[ê°€ìŠ¤ì‹ ë¬¸] {page} í˜ì´ì§€ í¬ë¡¤ë§ â†’ {url}")

        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("section#section-list ul.type1 > li"):
            title_a = li.select_one("h4.titles a")
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            href = title_a.get("href") or ""
            article_url = GAS_BASE_URL + href

            date_el = li.select_one("em.info.dated")
            date_str = normalize_gas_date(date_el.get_text(strip=True) if date_el else "")

            # í•„í„°: ì œëª© ê¸°ì¤€
            if not contains_hydrogen_keyword(title):
                continue

            body = extract_article_body(article_url)
            summary_3 = summarize_body(body, max_lines=3)
            tags = make_tags(title, body)

            results.append(
                {
                    "date": date_str,
                    "source": "ê°€ìŠ¤ì‹ ë¬¸",
                    "title": title,
                    "url": article_url,
                    "summary_3lines": summary_3,
                    "body": body,
                    "tags": tags,
                }
            )

    return results


# ============================================
# 6. í¬ë¡¤ëŸ¬: ì „ê¸°ì‹ ë¬¸
# ============================================

def crawl_electimes(max_pages: int = 2) -> list[dict]:
    results: list[dict] = []

    for page in range(1, max_pages + 1):
        url = ELECT_LIST_URL.format(page=page)
        print(f"[ì „ê¸°ì‹ ë¬¸] {page} í˜ì´ì§€ í¬ë¡¤ë§ â†’ {url}")

        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for li in soup.select("#section-list ul.type > li.item"):
            title_a = li.select_one("div.view-cont h4.titles a.linked.replace-titles")
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            href = title_a.get("href") or ""
            if href.startswith("/"):
                article_url = ELECT_BASE_URL + href
            else:
                article_url = href

            date_el = li.select_one("div.view-cont em.replace-date")
            date_str = normalize_elect_date(
                date_el.get_text(strip=True) if date_el else ""
            )

            summary_el = li.select_one("div.view-cont p.lead a.replace-read")
            list_summary = summary_el.get_text(strip=True) if summary_el else ""

            # ì œëª© + ë¦¬ìŠ¤íŠ¸ ìš”ì•½ ê¸°ì¤€ í•„í„°
            filter_text = f"{title} {list_summary}"
            if not contains_hydrogen_keyword(filter_text):
                continue

            body = extract_article_body(article_url)
            summary_3 = summarize_body(body or list_summary, max_lines=3)
            tags = make_tags(title, body)

            results.append(
                {
                    "date": date_str,
                    "source": "ì „ê¸°ì‹ ë¬¸",
                    "title": title,
                    "url": article_url,
                    "list_summary": list_summary,
                    "summary_3lines": summary_3,
                    "body": body,
                    "tags": tags,
                }
            )

    return results


# ============================================
# 7. GPT-style 'ì—°ê²° ê¸°ì‚¬' ê°„ë‹¨ ì•Œê³ ë¦¬ì¦˜
# ============================================

def build_related_map(articles: list[dict], top_k: int = 3) -> None:
    """
    ì•„ì£¼ ë‹¨ìˆœí•œ ìœ ì‚¬ë„ ê³„ì‚°:
      - íƒœê·¸ ê²¹ì¹˜ëŠ” ê°œìˆ˜
      - ì œëª©ì— ê³µí†µìœ¼ë¡œ í¬í•¨ëœ ìˆ˜ì†Œ í‚¤ì›Œë“œ ê°œìˆ˜
    ìƒìœ„ top_k ê°œë¥¼ related ë¦¬ìŠ¤íŠ¸ì— URL ê¸°ì¤€ìœ¼ë¡œ ì €ì¥.
    """
    n = len(articles)
    for i in range(n):
        scores: list[tuple[float, int]] = []
        tags_i = set(articles[i].get("tags") or [])
        title_i = articles[i]["title"]

        for j in range(n):
            if i == j:
                continue

            tags_j = set(articles[j].get("tags") or [])
            title_j = articles[j]["title"]

            tag_score = len(tags_i & tags_j)
            kw_score = 0
            for kw in HYDROGEN_KEYWORDS:
                if kw in title_i and kw in title_j:
                    kw_score += 1

            score = tag_score * 2 + kw_score
            if score > 0:
                scores.append((score, j))

        scores.sort(reverse=True, key=lambda x: x[0])
        related_urls = [articles[j]["url"] for (_, j) in scores[:top_k]]
        articles[i]["related"] = related_urls


# ============================================
# 8. ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ ìƒì„±
# ============================================

from cardnews_image import make_cardnews_image


def generate_card_images(articles: list[dict], today: str) -> None:
    """
    ê° ê¸°ì‚¬ë§ˆë‹¤ 3ì¤„ ìš”ì•½ìœ¼ë¡œ ì¹´ë“œë‰´ìŠ¤ PNG ìƒì„±.
    ìƒì„±ëœ íŒŒì¼ ê²½ë¡œë¥¼ article["card_image"]ì— ì €ì¥.
    """
    today_dir = CARDS_ROOT / today
    today_dir.mkdir(parents=True, exist_ok=True)

    for idx, art in enumerate(articles, start=1):
        summary = art.get("summary_3lines") or art["title"]
        lines = summary.split("\n")
        # 3ì¤„ ë§ì¶”ê¸° (ëª¨ìë¼ë©´ ì œëª©/ë‚ ì§œ ì¶”ê°€)
        while len(lines) < 3:
            if len(lines) == 0:
                lines.append(art["title"])
            elif len(lines) == 1:
                lines.append(art["source"])
            else:
                lines.append(art["date"])

        filename = f"card_{idx:02d}.png"
        out_path = today_dir / filename

        make_cardnews_image(lines[:3], str(out_path))

        # JSON/HTMLì—ì„œ ì‚¬ìš©í•  ìƒëŒ€ ê²½ë¡œ (GitHub Pages ê¸°ì¤€)
        art["card_image"] = f"cards/{today}/{filename}"


# ============================================
# 9. ì˜¤ëŠ˜ì JSON ì €ì¥
# ============================================

def save_json(articles: list[dict], today: str) -> Path:
    DATA_DIR.mkdir(exist_ok=True)
    out_path = DATA_DIR / f"{today}.json"

    with out_path.open("w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    print(f"\nğŸŸ¢ JSON ì €ì¥ ì™„ë£Œ: {len(articles)}ê±´ â†’ {out_path}")
    return out_path


# ============================================
# 10. ë©”ì¸ ì‹¤í–‰
# ============================================

def main():
    today = datetime.now().strftime("%Y-%m-%d")
    print("=== í˜„ì¬ ë””ë ‰í† ë¦¬ êµ¬ì¡° ===")
    for p in Path(".").iterdir():
        print(p)
    print("=== í¬ë¡¤ëŸ¬ ì‹¤í–‰ ===")

    all_articles: list[dict] = []
    all_articles.extend(crawl_gasnews(max_pages=3))
    all_articles.extend(crawl_electimes(max_pages=3))

    # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ë§Œ í•„í„°ë§
    today_articles = [a for a in all_articles if a.get("date") == today]

    # GPT-style ê´€ë ¨ ê¸°ì‚¬ ë§í¬ ê³„ì‚°
    build_related_map(today_articles, top_k=3)

    # ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ ìƒì„±
    generate_card_images(today_articles, today)

    # JSON ì €ì¥
    save_json(today_articles, today)


if __name__ == "__main__":
    main()
